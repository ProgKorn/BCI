# -*- coding: utf-8 -*-
"""Bayesian_Causal_Inference_Depression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kFahaW6Y8H2jO2tPlOPSdFu72eU3wEWH

Import Data
"""

!pip install mne
import mne
from google.colab import drive
drive.mount('/content/drive')
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc
from sklearn.model_selection import KFold
from sklearn.feature_selection import mutual_info_classif
!pip install pyedflib
import pyedflib
import traceback
!pip install pgmpy
from pgmpy.models import BayesianNetwork
from pgmpy.estimators import MaximumLikelihoodEstimator, PC
from pgmpy.inference import VariableElimination
import seaborn as sns
! pip install mne-features
import mne_features

# Locate data folder and all files
data_folder = '/content/drive/My Drive/Colab Notebooks/Depression Data' # change last folder to data set
edf_files = [os.path.join(data_folder, f) for f in os.listdir(data_folder) if f.endswith('.edf')]

# print (edf_files)

"""Process Data"""

# Initialise lists for feature and label storage
features_processed = []
labels_processed = []

# Iterate over EEG data files
for file in edf_files:
    # Read EEG data from file as check for valid files
    raw = mne.io.read_raw_edf(file, preload=True)

    # Extract signals and channels into an array form
    n_channels = len(raw.ch_names) # Get number of channels
    signal_lbl = raw.ch_names # Get channel labels
    signals = raw.get_data() # Get signal data as NumPy array

    # Convert matrix into MNE raw object for processing
    meta = mne.create_info(ch_names=signal_lbl, sfreq=raw.info['sfreq'], ch_types='eeg')
    raw_object = mne.io.RawArray(signals, meta)

    # Filter all data to remove low and high frequency
    raw_object_filtered = raw_object.copy().filter(l_freq=0.5, h_freq=30)

    # Detect and interpolate bad channels in all data
    raw_object_bad_channels = raw_object_filtered.info['bads']
    raw_object_filtered.interpolate_bads(reset_bads=True)

    # Apply ICA for artifact removal to all data
    raw_object_ica = mne.preprocessing.ICA(n_components=10, random_state=97)
    raw_object_ica.fit(raw_object_filtered)

    # Apply ICA and get the processed data
    raw_object_cleaned = raw_object_ica.apply(raw_object_filtered)

    # Filter to extract Alpha (8 - 12 Hz)
    alpha_band = raw_object_cleaned.copy().filter(l_freq=8, h_freq=12)

    # Filter to extract Beta (13 - 30 Hz)
    beta_band = raw_object_cleaned.copy().filter(l_freq=13, h_freq=30)

    # Filter to extract Theta (4 - 7 Hz)
    theta_band = raw_object_cleaned.copy().filter(l_freq=4, h_freq=7)

    # Filter to extra Delta (0.5 - 3 Hz)
    delta_band = raw_object_cleaned.copy().filter(l_freq=0.5, h_freq=3)

    # Filter to extract Gamme (30 - 50 Hz)
    gamma_band = raw_object_cleaned.copy().filter(l_freq=30, h_freq=50)

    # Calculate power spectral density (PSD)
    psd_alpha = alpha_band.compute_psd(method='welch', tmin=10, tmax=20, fmin=0.5, fmax=30, picks=None)
    psd_beta = beta_band.compute_psd(method='welch', tmin=10, tmax=20, fmin=0.5, fmax=30, picks=None)
    psd_gamma = gamma_band.compute_psd(method='welch', tmin=10, tmax=20, fmin=0.5, fmax=30, picks=None)
    psd_theta = theta_band.compute_psd(method='welch', tmin=10, tmax=20, fmin=0.5, fmax=30, picks=None)
    psd_delta = delta_band.compute_psd(method='welch', tmin=10, tmax=20, fmin=0.5, fmax=30, picks=None)

    # Calculate Higuchi's Fractal Dimension
    HFD_alpha = mne_features.univariate.compute_higuchi_fd(alpha_band.get_data(), kmax=10)
    HFD_beta = mne_features.univariate.compute_higuchi_fd(beta_band.get_data(), kmax=10)

    # Extract the PSD values as NumPy arrays
    psd_alpha_values = psd_alpha.get_data()
    psd_beta_values = psd_beta.get_data()
    psd_gamma_values = psd_gamma.get_data()
    psd_theta_values = psd_theta.get_data()
    psd_delta_values = psd_delta.get_data()

    # Extract the HFD values as NumPy arrays
    HFD_alpha_values = HFD_alpha
    HFD_beta_values = HFD_beta

    # Append features and labels (Feature Selection)
    features_processed.append([np.mean(psd_alpha_values), np.mean(psd_beta_values), np.mean(psd_gamma_values),
                               np.mean(psd_theta_values), np.mean(psd_delta_values), np.mean(HFD_alpha_values),
                               np.mean(HFD_beta_values)])

    # Add label based on file name
    if 'H' in file:
        labels_processed.append("Healthy")
    else:
        labels_processed.append("Depressed")

    raw.close()

# Convert to DataFrame for Algorithm
features_processed_df = pd.DataFrame(features_processed, columns=['Alpha PSD', 'Beta PSD', 'Gamma PSD', 'Theta PSD', 'Delta PSD', 'HFD Alpha', 'HFD Beta'])
labels_processed = pd.DataFrame(labels_processed, columns=['Label'])

print(features_processed_df)
print(labels_processed)

# Save features and labels to CSV files
import os

# Specify the directory
save_dir = '/content/drive/My Drive/Colab Notebooks'

# Create the directory if it doesn't exist
os.makedirs(save_dir, exist_ok=True)

# Save features to CSV
features_file_path = os.path.join(save_dir, 'features_processed.csv')
features_processed_df.to_csv(features_file_path, index=False)
print(f"Features saved to: {features_file_path}")

# Save labels to CSV
labels_file_path = os.path.join(save_dir, 'labels_processed.csv')
labels_processed.to_csv(labels_file_path, index=False)
print(f"Labels saved to: {labels_file_path}")

"""Bayesian Analysis

Default Bayesian Inference Analysis
"""

import pandas as pd
import numpy as np
import networkx as nx
import matplotlib.pyplot as plt
import seaborn as sns
from pgmpy.estimators import HillClimbSearch, K2Score, BayesianEstimator
from pgmpy.models import BayesianNetwork
from pgmpy.inference import VariableElimination
from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, f1_score, roc_curve
from sklearn.feature_selection import mutual_info_classif
from sklearn.model_selection import KFold
from sklearn.preprocessing import KBinsDiscretizer
from imblearn.over_sampling import SMOTE

print("Debug: Starting script")

# Load and preprocess data
features_file_path = '/content/drive/My Drive/Colab Notebooks/features_processed.csv'
labels_file_path = '/content/drive/My Drive/Colab Notebooks/labels_processed.csv'

features_processed_df = pd.read_csv(features_file_path)
labels_processed = pd.read_csv(labels_file_path)

psd_columns = ['Alpha PSD', 'Beta PSD', 'Gamma PSD', 'Theta PSD', 'Delta PSD']

# Log transformation
for col in psd_columns:
    features_processed_df[col] = np.log10(features_processed_df[col] + 1e-6)

feature_columns = ['Alpha_Power', 'Beta_Power', 'Gamma_Power', 'Theta_Power', 'Delta_Power', 'HFD_Alpha', 'HFD_Beta']
data = pd.DataFrame({
    'Alpha_Power': features_processed_df['Alpha PSD'],
    'Beta_Power': features_processed_df['Beta PSD'],
    'Gamma_Power': features_processed_df['Gamma PSD'],
    'Theta_Power': features_processed_df['Theta PSD'],
    'Delta_Power': features_processed_df['Delta PSD'],
    'HFD_Alpha': features_processed_df['HFD Alpha'],
    'HFD_Beta': features_processed_df['HFD Beta'],
    'Depression': (labels_processed['Label'] == 'Depressed').astype(int)
})

data = data.replace([np.inf, -np.inf], np.nan).dropna()

# Feature selection
def select_features(X, y, k=5):
    selector = mutual_info_classif(X, y)
    selected_features = [feature_columns[i] for i in np.argsort(selector)[-k:]]
    return selected_features

# Function to create and train Bayesian Network
def create_and_train_bn(train_data, selected_features, n_bins=7, max_indegree=5):
    discretizer = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='quantile')
    discretizer.fit(train_data[selected_features])

    train_discretized = train_data.copy()
    train_discretized[selected_features] = discretizer.transform(train_data[selected_features])

    hc = HillClimbSearch(train_discretized)
    best_model = hc.estimate(
        scoring_method=K2Score(train_discretized),
        max_indegree=max_indegree,
        max_iter=int(1e5)
    )

    model = BayesianNetwork(best_model.edges())
    model.fit(train_discretized, estimator=BayesianEstimator, prior_type="BDeu", equivalent_sample_size=10)

    return model, discretizer

# Function for prediction
def predict_depression(model, discretizer, row, selected_features):
    evidence = dict(zip(selected_features, discretizer.transform([row[selected_features]])[0]))
    inference = VariableElimination(model)
    result = inference.query(['Depression'], evidence=evidence)
    return result.values[1]

# K-fold cross-validation
k_folds = 5
kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)

accuracies = []
aucs = []
feature_importances = []
confusion_matrices = []

for fold, (train_index, test_index) in enumerate(kf.split(data), 1):
    print(f"\nFold {fold}")

    train_data = data.iloc[train_index]
    test_data = data.iloc[test_index]

    # Apply SMOTE to handle class imbalance
    smote = SMOTE(random_state=42, sampling_strategy='auto', k_neighbors=5)
    X_train_resampled, y_train_resampled = smote.fit_resample(train_data[feature_columns], train_data['Depression'])
    train_data_resampled = pd.DataFrame(X_train_resampled, columns=feature_columns)
    train_data_resampled['Depression'] = y_train_resampled

    selected_features = select_features(train_data_resampled[feature_columns], train_data_resampled['Depression'])
    print("Selected features:", selected_features)

    model, discretizer = create_and_train_bn(train_data_resampled, selected_features)

    # Predictions and evaluation
    y_pred = test_data.apply(lambda row: predict_depression(model, discretizer, row, selected_features) > 0.5, axis=1)
    y_true = test_data['Depression']

    accuracy = accuracy_score(y_true, y_pred)
    accuracies.append(accuracy)

    y_prob = test_data.apply(lambda row: predict_depression(model, discretizer, row, selected_features), axis=1)
    auc = roc_auc_score(y_true, y_prob)
    aucs.append(auc)

    # Feature Importance
    mi = mutual_info_classif(test_data[selected_features], y_true)
    feature_importance = dict(zip(selected_features, mi))
    feature_importances.append(feature_importance)

    # Confusion Matrix
    cm = confusion_matrix(y_true, y_pred)
    confusion_matrices.append(cm)

    print(f"Fold {fold} Accuracy: {accuracy}")
    print(f"Fold {fold} AUC: {auc}")
    print(f"Fold {fold} Feature Importance: {feature_importance}")
    print(f"Fold {fold} Confusion Matrix:\n", cm)

# Overall results
print("\nOverall Results:")
print(f"Average Accuracy: {np.mean(accuracies):.4f} (+/- {np.std(accuracies):.4f})")
print(f"Average AUC: {np.mean(aucs):.4f} (+/- {np.std(aucs):.4f})")

# Aggregate feature importance
all_features = set().union(*feature_importances)
avg_importance = {feature: np.mean([d.get(feature, 0) for d in feature_importances]) for feature in all_features}
print("Average Feature Importance:", avg_importance)

# Average Confusion Matrix
avg_cm = np.mean(confusion_matrices, axis=0)
print("Average Confusion Matrix:")
print(avg_cm)

# Visualizations
plt.figure(figsize=(20, 15))

# Accuracy Distribution
plt.subplot(2, 2, 1)
sns.boxplot(accuracies)
plt.title('Accuracy Distribution')
plt.ylabel('Accuracy')

# AUC Distribution
plt.subplot(2, 2, 2)
sns.boxplot(aucs)
plt.title('AUC Distribution')
plt.ylabel('AUC')

# Average Feature Importance
plt.subplot(2, 2, 3)
features, importances = zip(*sorted(avg_importance.items(), key=lambda x: x[1], reverse=True))
sns.barplot(x=list(features), y=list(importances))
plt.title('Average Feature Importance')
plt.xticks(rotation=45, ha='right')
plt.ylabel('Importance')

# Average Confusion Matrix
plt.subplot(2, 2, 4)
sns.heatmap(avg_cm, annot=True, fmt='.2f', cmap='Blues')
plt.title('Average Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')

plt.tight_layout()
plt.savefig('bayesian_network_results.png')
plt.show()

################

# Train final model on entire dataset
final_selected_features = select_features(data[feature_columns], data['Depression'])
final_model, final_discretizer = create_and_train_bn(data, final_selected_features)

print("\nFinal Model:")
print("Selected Features:", final_selected_features)
print("Model Edges:", final_model.edges())

# Evaluate final model
y_pred = data.apply(lambda row: predict_depression(final_model, final_discretizer, row, final_selected_features) > 0.5, axis=1)
y_true = data['Depression']
y_prob = data.apply(lambda row: predict_depression(final_model, final_discretizer, row, final_selected_features), axis=1)

# Calculate metrics
accuracy = accuracy_score(y_true, y_pred)
auc = roc_auc_score(y_true, y_prob)
cm = confusion_matrix(y_true, y_pred)
f1 = f1_score(y_true, y_pred)

print(f"\nFinal Model Performance:")
print(f"Accuracy: {accuracy:.4f}")
print(f"AUC: {auc:.4f}")
print(f"F1 Score: {f1:.4f}")
print("Confusion Matrix:")
print(cm)

# Visualizations for final model
plt.figure(figsize=(20, 15))

# ROC Curve
plt.subplot(2, 2, 1)
fpr, tpr, _ = roc_curve(y_true, y_prob)
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")

# Confusion Matrix
plt.subplot(2, 2, 2)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')

# Feature Importance
plt.subplot(2, 2, 3)
mi = mutual_info_classif(data[final_selected_features], y_true)
feature_importance = dict(zip(final_selected_features, mi))
features, importances = zip(*sorted(feature_importance.items(), key=lambda x: x[1], reverse=True))
sns.barplot(x=list(features), y=list(importances))
plt.title('Feature Importance')
plt.xticks(rotation=45, ha='right')
plt.ylabel('Importance')

# Bayesian Network Structure
plt.subplot(2, 2, 4)
graph = nx.DiGraph()
graph.add_edges_from(final_model.edges())
pos = nx.spring_layout(graph)
nx.draw(graph, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold')
plt.title('Bayesian Network Structure')

plt.tight_layout()
plt.savefig('final_model_results.png')
plt.show()



def sensitivity_analysis(model, discretizer, data, feature, values):
    results = []
    original_value = data[feature].copy()
    for value in values:
        data[feature] = value
        y_prob = data.apply(lambda row: predict_depression(model, discretizer, row, final_selected_features), axis=1)
        mean_prob = y_prob.mean()
        results.append(mean_prob)
    data[feature] = original_value
    return results

# Perform sensitivity analysis for Gamma and Theta
#gamma_values = np.linspace(data['Gamma_Power'].min(), data['Gamma_Power'].max(), 20)
#theta_values = np.linspace(data['Theta_Power'].min(), data['Theta_Power'].max(), 20)

#Expanding the range by +-10% to make it more noticable

gamma_range = data['Gamma_Power'].max() - data['Gamma_Power'].min()
gamma_values = np.linspace(data['Gamma_Power'].min() - 0.1 * gamma_range,
                           data['Gamma_Power'].max() + 0.1 * gamma_range, 20)

theta_range = data['Theta_Power'].max() - data['Theta_Power'].min()
theta_values = np.linspace(data['Theta_Power'].min() - 0.1 * theta_range,
                           data['Theta_Power'].max() + 0.1 * theta_range, 20)
########

gamma_sensitivity = sensitivity_analysis(final_model, final_discretizer, data, 'Gamma_Power', gamma_values)
theta_sensitivity = sensitivity_analysis(final_model, final_discretizer, data, 'Theta_Power', theta_values)

# Visualize sensitivity analysis results
plt.figure(figsize=(15, 6))

plt.subplot(1, 2, 1)
plt.plot(gamma_values, gamma_sensitivity)
plt.title('Sensitivity Analysis: Gamma Power')
plt.xlabel('Gamma Power')
plt.ylabel('Mean Predicted Probability of Depression')

plt.subplot(1, 2, 2)
plt.plot(theta_values, theta_sensitivity)
plt.title('Sensitivity Analysis: Theta Power')
plt.xlabel('Theta Power')
plt.ylabel('Mean Predicted Probability of Depression')

plt.tight_layout()
plt.savefig('sensitivity_analysis.png')
plt.show()

# After performing the sensitivity analysis and before plotting the results, add:

print("\nDetailed Sensitivity Analysis Results:")

print("\nGamma Power Sensitivity:")
for value, effect in zip(gamma_values, gamma_sensitivity):
    print(f"Gamma Power: {value:.6f}, Mean Predicted Probability: {effect:.6f}")

print("\nTheta Power Sensitivity:")
for value, effect in zip(theta_values, theta_sensitivity):
    print(f"Theta Power: {value:.6f}, Mean Predicted Probability: {effect:.6f}")

# Then continue with the plotting and other parts of the code

print("\nSensitivity Analysis:")
print("Gamma Power range:", gamma_values[0], "to", gamma_values[-1])
print("Theta Power range:", theta_values[0], "to", theta_values[-1])
print("Gamma Power effect range:", min(gamma_sensitivity), "to", max(gamma_sensitivity))
print("Theta Power effect range:", min(theta_sensitivity), "to", max(theta_sensitivity))


# Check the final model
try:
    final_model.check_model()
    print("\nModel check passed: The model is consistent.")
except Exception as e:
    print("\nModel check failed:", e)

# Print CPDs for the final model
print("\nConditional Probability Distributions (CPDs) for the Final Model:")
for node in final_model.nodes():
    print(f"\nCPD for {node}:")
    cpd = final_model.get_cpds(node)
    print(cpd)



# # Save final model
# import pickle

# # Define the path where you want to save the model
# model_path = '/content/drive/My Drive/Colab Notebooks/BCIModel_Mk1.pkl'

# # Save the model
# with open(model_path, 'wb') as f:
#     pickle.dump((final_model, final_discretizer, final_selected_features), f)

# print(f"Final model saved as '{model_path}'")

print("Debug: Script completed")

"""## Importing and infering unseen data"""

import time
import mne
import numpy as np
import pandas as pd
import mne_features
import pickle
from pgmpy.inference import VariableElimination
import os

def process_single_file(file_path):
    # Read EEG data from file
    raw = mne.io.read_raw_edf(file_path, preload=True)

    # Extract signals and channels into an array form
    n_channels = len(raw.ch_names)
    signal_lbl = raw.ch_names
    signals = raw.get_data()

    # Convert matrix into MNE raw object for processing
    meta = mne.create_info(ch_names=signal_lbl, sfreq=raw.info['sfreq'], ch_types='eeg')
    raw_object = mne.io.RawArray(signals, meta)

    # Filter all data to remove low and high frequency
    raw_object_filtered = raw_object.copy().filter(l_freq=0.5, h_freq=30)

    # Detect and interpolate bad channels in all data
    raw_object_bad_channels = raw_object_filtered.info['bads']
    raw_object_filtered.interpolate_bads(reset_bads=True)

    # Apply ICA for artifact removal to all data
    raw_object_ica = mne.preprocessing.ICA(n_components=10, random_state=97)
    raw_object_ica.fit(raw_object_filtered)

    # Apply ICA and get the processed data
    raw_object_cleaned = raw_object_ica.apply(raw_object_filtered)

    # Filter to extract frequency bands
    alpha_band = raw_object_cleaned.copy().filter(l_freq=8, h_freq=12)
    beta_band = raw_object_cleaned.copy().filter(l_freq=13, h_freq=30)
    theta_band = raw_object_cleaned.copy().filter(l_freq=4, h_freq=7)
    delta_band = raw_object_cleaned.copy().filter(l_freq=0.5, h_freq=3)
    gamma_band = raw_object_cleaned.copy().filter(l_freq=30, h_freq=50)

    # Calculate power spectral density (PSD)
    psd_alpha = alpha_band.compute_psd(method='welch', tmin=10, tmax=20, fmin=0.5, fmax=30, picks=None)
    psd_beta = beta_band.compute_psd(method='welch', tmin=10, tmax=20, fmin=0.5, fmax=30, picks=None)
    psd_gamma = gamma_band.compute_psd(method='welch', tmin=10, tmax=20, fmin=0.5, fmax=30, picks=None)
    psd_theta = theta_band.compute_psd(method='welch', tmin=10, tmax=20, fmin=0.5, fmax=30, picks=None)
    psd_delta = delta_band.compute_psd(method='welch', tmin=10, tmax=20, fmin=0.5, fmax=30, picks=None)

    # Calculate Higuchi's Fractal Dimension
    HFD_alpha = mne_features.univariate.compute_higuchi_fd(alpha_band.get_data(), kmax=10)
    HFD_beta = mne_features.univariate.compute_higuchi_fd(beta_band.get_data(), kmax=10)

    # Extract the PSD and HFD values as NumPy arrays
    psd_values = {
        'Alpha_Power': np.mean(psd_alpha.get_data()),
        'Beta_Power': np.mean(psd_beta.get_data()),
        'Gamma_Power': np.mean(psd_gamma.get_data()),
        'Theta_Power': np.mean(psd_theta.get_data()),
        'Delta_Power': np.mean(psd_delta.get_data()),
        'HFD_Alpha': np.mean(HFD_alpha),
        'HFD_Beta': np.mean(HFD_beta)
    }

    raw.close()

    return psd_values

def predict_depression(model, discretizer, row, selected_features):
    evidence = dict(zip(selected_features, discretizer.transform([row[selected_features]])[0]))
    inference = VariableElimination(model)
    result = inference.query(['Depression'], evidence=evidence)
    return result.values[1]

# Load the saved model
model_path = '/content/drive/My Drive/Colab Notebooks/BCIModel_Final_Mk1.pkl'
with open(model_path, 'rb') as f:
    final_model, final_discretizer, final_selected_features = pickle.load(f)

# Directory containing EEG files
eeg_dir = '/content/drive/My Drive/Colab Notebooks/Depression Tester/'

def show_files():
    print("\nEEG files in the directory:")
    eeg_files = [f for f in os.listdir(eeg_dir) if f.endswith('.edf')]
    for file in eeg_files:
        print(file)
    print()

print("EEG Depression Inference System")
print("Enter 'quit' to exit the program.")
print("Enter 'show' to display all EEG files in the directory.")

while True:
    filename = input("\nEnter the name of the EEG file (including .edf extension), 'show', or 'quit': ")

    if filename.lower() == 'quit':
        print("Exiting the program.")
        break

    if filename.lower() == 'show':
        show_files()
        continue

    file_path = os.path.join(eeg_dir, filename)

    if not os.path.exists(file_path):
        print(f"Error: File '{filename}' not found in the directory.")
        continue

    try:
        start_time = time.time()

        processed_data = process_single_file(file_path)

        # Apply log transformation to PSD values
        psd_columns = ['Alpha_Power', 'Beta_Power', 'Gamma_Power', 'Theta_Power', 'Delta_Power']
        for col in psd_columns:
            processed_data[col] = np.log10(processed_data[col] + 1e-6)

        # Create a DataFrame with the processed data
        data = pd.DataFrame([processed_data])

        # Perform inference
        depression_probability = predict_depression(final_model, final_discretizer, data.iloc[0], final_selected_features)

        end_time = time.time()
        inference_time = end_time - start_time

        prediction = 'Depressed' if depression_probability > 0.4 else 'Healthy'

        print("\nInference Results:")
        print(f"File Name: {filename}")
        print(f"Depression Probability: {depression_probability:.4f}")
        print(f"Prediction: {prediction}")
        print(f"Time taken: {inference_time:.2f} seconds")

    except Exception as e:
        print(f"Error processing {filename}: {str(e)}")

print("Program terminated.")